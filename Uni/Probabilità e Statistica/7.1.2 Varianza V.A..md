>[!def] Varianza
>Sia $X$ una v.a. su $(\Omega, F, P)$ che ammetta valor medio finito.
>La _varianza_ di $X$ è data da
>$$
>var(X) := E[(X - E[X])^2]
>$$

Osservazioni
1. Se $X$ ammette **valor medio finito**, allora $var(X)$ è ben definita in $[0, \infty]$.
   La varianza di $X$ è definita se e solo se $X^2$ ammette valor medio finito
2. Confronto con la varianza campionaria di un campione $(x_{i})_{i \in \{ 1 \dots n \}} \subseteq \mathbb{R}$:
   $$
   s^2 = \frac{1}{n - 1} \cdot \sum_{i = 1}^n (x_{i} - \overline x)^2
   $$
   A parte il prefattore $\frac{1}{n-1}$ al posto di $\frac{1}{n}$, $s^2$ è la media campionaria degli scarti quadratici 
   Analogia con la definizione di varianza:
   - $(X-E[X])^2$: scarti quadratici
   - $E(X-E[X])^2]$: valor medio scarti quadratici
1. Se $X^2$ ammette valor medio finito, allora
   $$
   \begin{align}
var(X) &= E[(X - E[X])^2] \\
&= E[X^2 - 2 \cdot X \cdot E[X] + E[X]^2] \\
&= E[X^2] - E[2 \cdot X \cdot E[X]] + E[X]^2 \quad (*) \\
&= E[X^2] - 2\cdot E[X] \cdot E[X] + E[X]^2 \\
&= E[X^2] - E[X]^2 \\
var(X) &= E[X^2] - E[X^2]
\end{align}
   $$
   $$
   (*) \quad E[2 \cdot X\cdot E[X]] = E[2E[X]\cdot X] = 2E[X] \cdot E[X]
   $$
   dove $2E[X]$ è una costante $\alpha$ quindi si applica la linearità
2. ==$var(c\cdot X) = c^2 \cdot var(X)$, $\forall c \in \mathbb{R}$==






Sia $X$ una v.a. con densità $p_{X}$ tale che ==$\sum_{x \in \mathbb{R}}x^2 p_{X}(x) < \infty$ -> $X^2$ ammette valor medio finito==
Grazie all'osservazione 3.
$$
var(X) = E[X^2] - E[X]^2=\sum_{x\in\mathbb{R}}x^2\cdot p_{X}(x) -\left( \sum_{x \in \mathbb{R}}x \cdot p_{X}(x) \right)^2
$$

>[!info] Osservazione
>Se $X = c$ q.c. (quasi certamente), per una costante $c \in \mathbb{R}$, cioè se esiste $c \in \mathbb{R}$ tale che $P(X = c) = 1$ allora $var(X) = 0$
>Infatti: $var(X) = 0$ se e solo se $X$ q.c. costante

>[!example] Esempio: varianza di una v.a. di Bernoulli
>Sia $X\sim \text{Ber}(q)$ con $q \in [0,1]$
>-> 
>$$
>p_{X}(x) = \begin{cases}
> q & \text{se } x = 1 \\
> 1 - q & \text{se } x=0 \\
> 0 & \text{altrimenti}
\end{cases}
>$$
>-> 
>$$
>\begin{align}
>E[X] &= 1 \cdot q + 0 \cdot (1- q)=q
>E[X^2] &= 1^2 \cdot q + 0^2 \cdot (1 - q) = q
>
\end{align}
>$$
>-> 
>$$
>\begin{align}
>var(X) &= E[X^2] - E[X]^2 = q - q^2
>var(X) &= q \cdot (1 - q)
\end{align}
>$$
>> Nota
>> $[0,1] \ni q \mapsto q\cdot(1-q)$ è una funzione non-negativa che assume il suo massimo in $q=\frac{1}{2}$ -> $var(X) \in \left[0, \frac{1}{2}\right]$

>[!teo] Disuguaglianza di Markov-Chebyshev
>==Permette di stimare la probabilità di valori "grandi" di una v.a. non-negativa e di valori "lontani dalla media" per v.a. con varianza finita.==
>
>---
> Sia $X$ una v.a. su $(\Omega, F, P)$
> ##### Markov
> Se $X \geq 0$, allora $\forall \epsilon > 0$
> $$
> P(X \geq \epsilon) \leq \frac{E[X]}{\epsilon}
> $$
> La versione generalizzata è:
> Se $X\geq 0$ e $f: [0, \infty) \mapsto [0, \infty)$ **crescente** tale che $f(x) > 0$ se $x > 0$ allora $\forall \epsilon > 0$
> $$
> P(X \geq \epsilon) \leq \frac{E[f(X)]}{f(\epsilon)}
> $$
> ##### Chebyshev
> Se $X$ ammette valor medio finito, allora $\forall \epsilon > 0$
> $$
> P(|X - E[X]|\geq \epsilon) \leq \frac{var(X)}{\epsilon^2}
> $$

## Aprossimazione di Poisson e Leggei dei piccoli numeri
>[!important] Ricorda
>Una v.a. binomiale di parametri $n,q$ dà il numero di successi in $n$ prove ripetute ed indipendenti con probab. di successo di una singola prova uguale a $q$

Supponiamo ora di voler contare il numero di arrivi (di clienti / messaggi / pacchetti) in un intervallo di tempo $[0,1]$ (1 minuto, 1 ora, ...)
Dividiamo l'intervallo di in $n$ intervallini di uguale lunghezza ($\frac{1}{n}$) con $n$ grande.
Sia $q_{n,k}$ la probabilità di vedere un arrivo nell'intervallino $k$-esimo.
Supponiamo anche che gli arrivi siano omogenei nel tempo:
$$
q_{n,k} = q_{n}
$$
Se $n$ è grande, possiamo supporre di vedere al massimo un arrivo a intervallino
inoltre, la pobabilità di un arrivo dovrebbe essere proporzionale alla lunghezza degli intervallini:
$$
q_{n} \approx \frac{\lambda}{n}, \text{ per una costante } \lambda > 0
$$
Se $S_{n} \sim \text{Bin}(n,q_{n})$ e $q_{n} \approx \frac{\lambda}{n}$, $n$ grande allora $S_{n}$ conta all'incirca il numero di arrivi in $[0,1]$
>[!question] Cosa succede per $n \to \infty$?
>Per ipotesi, $n \cdot q_{n} \to \lambda$

>[!teo] Legge Dei Piccoli Numeri
> Sia $(q_{n})_{n\in \mathbb{N}} \subset [0,1]$ tale che $n \cdot q_{n} \to \lambda$( dunque $q_{n} \approx \frac{\lambda}{n}$) per una costante $\lambda > 0$
> Sia:
> - $p_{n}$ la densità discreta della $\text{Bin}(n,q_{n})$
> - $\hat{p}_{\lambda}$ la denstià discreta della $\text{Poiss}(\lambda)$
> 
> > Ricorda
> > $$
> > \begin{align}
> > p_{n}(k) &= \begin{cases}
> > \binom{n}{k} \cdot q_{n}^k \cdot (1- q_{n})^{n - k} & \text{se } k \in \{ 0 \dots n \} \\
> > 0 & \text{altrimenti}
> >\end{cases} \\
> > \hat{p}_{\lambda}(k) &= \begin{cases}
> > e^{-\lambda} \cdot \frac{\lambda^k}{k!} & \text{se } k \in \mathbb{N}_{0} \\
> > 0 & \text{altrimenti}
> >\end{cases}
> >\end{align}
> > $$
> 
> Allora 
> $$
> \sum_{k = 0}^\infty |p_{k}(k) - \hat{p}_{\lambda}(k)| \to 0
> $$
> Questo pk $p_{n}$ superato $n$ continuerà a dare sempre 0, mentre $\hat{p}_{\lambda}$ è una funzione che tende a 0 con $k \to \infty$
> ==**Stima dell'errore**==
> $$
> \sum_{k = 0}^\infty |p_{k}(k) - \hat{p}_{\lambda}(k)| \leq 2 (n \cdot q_{n}^2 + |\lambda - n \cdot q_{n}|)
> $$

##### Applicazione approssimazione di Poisson
Sia $S \sim \text{Bin}(n,q)$ con $n$ "grande" e $q$ "piccolo"
Allora $S$ ha distribuzione vicina alla distribuzione di Poisson di parametro $\lambda=n\cdot q$
Di conseguenza, $\forall B \subseteq \mathbb{R}$
$$
P(S \in \mathbb{R}) = \sum_{x \in B} p_{s}(x) \approx \sum_{x \in B} \hat{p}_{\lambda}(x) = P(Y \in B)
$$
Dove $Y \sim \text{Poiss}(\lambda), \lambda = n \cdot q$.
In particolare $B = (-\infty, x]:\forall x \in \mathbb{R}:P(S \leq x) \approx P(Y \leq x)$, dove $S \sim \text{Bin}(n,q)$ e $Y \sim \text{Poiss}(n \cdot q)$.

###### Vantaggi dell'approssimazione di Poisson
- Probabilità in termini della distribuzione di Poisson più facili da calcolare rispetto alla binomiale
- La distribuzione di Poisson è determinata da un solo parametro, quella binomiale invece ne richiede due
- Il parametro $\lambda$ della distribuzione di Poisson è facile da stimare:
  Sia $Y \sim \text{Poiss}(\lambda)$ Allora $p_{Y} = \hat{p}_{\lambda}$ e
  $$
  \begin{align}
E[Y] &= \sum_{k \in \mathbb{N}_{0}}k \cdot \hat{p}_{\lambda}(k) \\
&= \sum_{k = 0}^\infty k \cdot e^{-\lambda} \cdot \frac{\lambda^k}{k!} \\
&= e^{-\lambda} \cdot \sum_{k = 1}^\infty k \cdot \frac{\lambda^k}{k!} \\
&= e^{-\lambda} \cdot \lambda \cdot \sum_{k = 1}^\infty \frac{\lambda^{k-1}}{(k-1)!} \\
&= e^{-\lambda} \cdot \lambda \cdot \sum_{k = 0}^\infty \frac{\lambda^k}{k!} \\
&\sum_{k = 0}^\infty \frac{\lambda^k}{k!} = e^\lambda\\
&= \lambda
\end{align}
  $$
-> $E[Y] = \lambda$ se $Y \sim \text{Poiss}(\lambda)$ -> basta stimare il valor medio!!