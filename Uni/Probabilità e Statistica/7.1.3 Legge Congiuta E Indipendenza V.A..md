> **Ricorda**
> Sia $X$ una v.a. su $(\Omega, F, P)$
> La legge (o distribuzione) di $X$ è la misura di probabilità $P_{X}$ su $\mathcal B(\mathbb{R})$ data da
> $$
> P_{X}(B) := P(X\in B) \quad B \in \mathcal{B}(\mathbb{R})
> $$
> Inoltre $P_{X}$ è determinata dalla probabilità degli insiemi della forma $B = (-\infty, b], b \in \mathbb{R}$ quindi $P_{X}$ è determinata da
> $$P_{X}((-\infty, b]) = P(X \leq b), \quad b \in \mathbb{R}$$

## Legge Congiunta

Quando abbiamo due (o più) v.a. sono spesso di interesse eventi che dipendono **congiuntamente** da queste v.a.
>[!example] Esempio
>Lancio di due dadi
>- $X$ punteggio segnato da primo dado
>- $Y$ punteggio segnato da secondo dado
>Eventi che dipendono congiuntamente da $X$ e $Y$ sono ad esempio:
>$A_{1}$ = "primo dado segna 3, il secondo 2" -> $A_{1} = \{ w \in \Omega : X(w) = 3\} \cap \{ w \in \Omega : Y (w) = 2 \} = \{ X = 3 \} \cap \{ Y = 2 \}$
>$A_{2}$ = "somma dei due dadi $\geq 5$ " -> $A_{2} = \{ w \in \Omega : X(w) + Y(w) \geq 5\} = \{ X + Y \geq 5 \}$
>Gli eventi $A_{1}, A_{2}$ son della forma $\{ (X,Y) \in B\}$ per un $B \subseteq \mathbb{R}^2$:
>- $A_{1} = \{ (X, Y) \in B\}$ con $B = \{ 3 \} \times \{ 2 \}$
>- $A_{2} = \{ (X,Y) \in B\}$ con $B = \{ (x,y) \in \mathbb{R}^2:x + y \geq 5\}$

L'esempio dei dadi appena svolto motiva la seguente definizione
>[!def] Legge Congiunta / Distribuzione congiunta
>###### Con 2 V.A.
>Siano $X, Y$ v.a. su $(\Omega, F, P)$.
>La *legge congiunta (o distribuzione congiunta)* di $X,Y$ è la misura di probabilità $P_{(X,Y)}$ su $\mathcal B(\mathbb {R^2})$ data da
>$$P_{(X, Y)}(B) := P((X,Y) \in B) \quad B \in \mathcal B(\mathbb {R^2})$$
>>[!important] Importante
>>L'ordine delle v.a. è importante
>>$$P_{(X,Y)} \neq P_{(Y,X)}$$
>>Infatti se nell'esempio dei due dadi il primo fosse truccato e il secondo equilibrato allora $P_{(X,Y)}\neq P_{(Y,X)}$.
>
>---
>###### Con d V.A.
>Siano $X_{1} \dots X_{d}$ v.a. su $(\Omega, F, P)$
>La _legge congiunta (o distribuzione congiunta)_ di $X_{1} \dots X_{d}$ è la misura di probabilità $P_{(X_{1}\dots X_{d})}$ su $\mathcal B(\mathbb{R}^d)$ data da
>$$P_{(X_{1} \dots X_{d})}(B) := P((X_{1} \dots X_{d}) \in B) \quad B \in \mathcal B(\mathbb{R}^d)$$
>> La distribuzione congiunta di $X_{1} \dots X_{d}$ è la legge del vettore aleatorio $X_{1} \dots X_{d}$
>> >[!def] Vettore Aleatorio d-dimensionale
>> >Un vettore aleatorio d-dimensionale su $(\Omega, F, P)$ è una funzione $X : \Omega \to \mathbb{R}^d$ tale che, $\forall i \in \{ 1 \dots d \}$, $X_{i}$ è una v.a. su $(\Omega, F, P)$, dove $X_{i}$ è la componente $i$-esima di $X = \{ X_{1} \dots X_{d} \}$.
>> 
>
>==La distribuzione congiunta descrive non solo il comportamento statistico delle singole v.a. (componenti del vettore aleatorio) ma anche le dipendenza tra esse.== Un caso "estremo" ma particolarmente importante è quello di v.a. indipendenti.

## Indipendenza
>[!def] V.A. Indipendenti (come famiglia)
>Siano $X_{1} \dots X_{n}$ v.a. su $(\Omega, F, P)$.
>Allora $X_{1} \dots X_{n}$ si dicono **indipendenti** (come famiglia)  se **per ogni scelta** di $B_{1} \dots B_{n} \in \mathcal B(\mathbb{R})$ si ha
>$$
>P(X_{1} \in B_{1} \dots X_{n} \in B_{n}) = P_{(X_{1} \dots X_{n})}(B_{1} \times \dots \times B_{n}) = \prod_{i = 1}^n P(X_{i} \in B_{i}) = \prod_{i = 1}^n P_{X_{i}}(B_{i})
>$$

**Osservazioni**
1. $X_{1} \dots X_{n}$ sono indipendenti **se e solo se** gli eventi $\{ X_{1} \in B_{1} \}, \dots,\{ X_{n} \in B_{n} \}$ sono indipendenti (come famiglia) per ogni scelta di $B_{1}, \dots, B_{n} \in \mathcal B(\mathbb{R})$
2. Se $X_{1} \dots X_{n}$ sono indipendenti, allora anche $X_{i}, X_{j}$ sono indipendenti per $i \neq j$
3. $X_{1} \dots X_{n}$ sono indipendenti **se e solo se** la loro distribuzione congiunta ha **forma prodotto**, cioè
   $$
   P_{(X_{1} \dots X_{n})}(B_{1} \times \dots \times B_{n}) = \prod_{i = 1}^n P_{X_{i}}(B_{i})
   $$
   Per ogni scelta di $B_{1}, \dots, B_{n} \in \mathcal B(\mathbb{R})$

==Se $X_{1} \dots X_{n}$ sono **indipendenti**, allora le loro leggi "marginali" determinano la legge congiunta==.

>[!important] Criterio per l'indipendenza
>Siano $X_{1} \dots X_{n}$ v.a. su $(\Omega, F, P)$.
>Allora $X_{1} \dots X_{n}$ sono indipendenti **se e solo se** per ogni scelta di $b_{1},\dots, b_{n} \in \mathbb{R}$ si ha
>$$
>P(X_{1} \leq b_{1}, \dots, X_{n} \leq b_{n}) = \prod_{i = 1}^n P(X_{i} \leq b_{i})
>$$

> L'indipendenza semplifica il calcolo del valor medio di prodotti di v.a.

>[!info] Valor medio e indipendenza
>Siano $X, Y$ v.a. su $(\Omega, F, P)$ con valor medio finito.
>Se $X, Y$ sono **indipendenti** allora
>$$
>E[X \cdot Y] = E[X] \cdot E[Y]
>$$
>==**In generale: $E[X \cdot Y] \neq E[X] \cdot E[Y]$**==

>[!info] L'indipendenza di v.a. si propaga sotto trasformazioni deterministiche
>Siano $X, Y, Z$ v.a. su $(\Omega, F, P)$, e siano $g: \mathbb{R}^2 \to \mathbb{R}$, $h : \mathbb{R} \to \mathbb{R}$ funzioni misurabili.
>Se $X, Y, Z$ sono indipendenti, allora $g(X,Y), h(Z)$ sono **indipendenti**
>>[!example] Esempio
>>$X, Y, Z$ punteggi segnati da tre dadi "indipendenti"
>>Allora $X + Y, \perp_{\{ 2k + 1: k \in \mathbb{N}_{0} \}}(Z)$ sono indipendenti
>>Qui: $g(x, y):= x + y, \quad h(z) := \perp_{\{ 2k+1: k \in \mathbb{N}_{0} \}}(z)$ con $x, y, z \in \mathbb{R}$

Anche se più in generale si ha
>[!important] Indipendenza a blocchi e per trasformazioni
>Siano $X_{1} \dots X_{n}$ v.a. e sia $(t_{k})_{k \in \{ 1 \dots L \}}$ una partizione dell'insieme degli indici $\{ 1 \dots n \}$ (cioè $\bigcup_{k = 1}^L t_{k} = \{ 1 \dots n \}$, $t_{k} \cap t_{l} = \emptyset$ se $k \neq l$, $t_{k} \neq \emptyset$)
>Per $k \in \{ 1 \dots L \}$, sia $f_{k}:\mathbb{R}^{|t_{k}|} \to \mathbb{R}$ una funzione misurabile.
>Se $X_{1} \dots X_{n}$ sono **indipendenti**, allora sono indipendenti che le v.a.
>$$
>f_{1}(X_{i}: i \in t_{1}), \dots, f_{l}(X_{i}:i \in t_{l})
>$$

## Caso Discreto
>[!def] Vettore Aleatorio Discreto
>Sia $X = (X_{1} \dots X_{n})$ un vettore aleatorio d-dim. su $(\Omega, F, P)$.
>Allora $X$ si dice discreto se $\mathrm{Im}(X) := \{ X(w) : w \in \Omega \}$ è al più numerabile.
>> Nota:
>> Un vettore aleatorio $X = (X_{1} \dots X_{n})$ è discreto **se e solo se** le componenti $X_{1} \dots X_{n}$ sono delle v.a. discreta.

==La distribuzione (o legge) di un vettore aleatorio discreto è determinata dalla sua densità discreta==.

>[!def] Densità Discreta di Vettore Aleatorio Discreto
>Sia $X=(X_{1} \dots X_{d})$ un vettore aleatorio discreto su $(\Omega, F, P)$.
>La **densità discreta** di $X$ è la funzione $p_{X}: \mathbb{R}^d \to [0,1]$ data da
>$$p_{X}(z) := P(X = z) = P(X_{1} = z_{1}, \dots, X_{d} = z_{d}) \quad z \in \mathbb{R}^d$$
>> Nota:
>> $p_{X}(z) = 0$ se $z \not\in \mathrm{Im}(X)$ ---> $\sum_{z \in \mathbb{R}^d}p_{X}(z) = \sum_{z \in \mathrm{Im}(X)}p_{X}(z) = 1$

La densità discreta di un vettore aleatorio discreto $X$ determina la distribuzione:
$$
P_{X}(B) = \sum_{z \in B} p_{X}(z) \quad B \in \mathcal B(\mathbb{R}^d)
$$
>[!question] Legame tra densità discreta di $X$ e densità discreta delle componenti $X_i$?
>
>>[!def] Densità Discreta Congiunta
>>La densità discreta del vettore aleatorio $X$ si dice **densità discreta congiunta** delle v.a. $X_{1}, \dots, X_{d}$
>
>>[!def] Densità Discrete Marginali
>>Le densità discrete delle componenti $X_{1}, \dots, X_{d}$ si dicono **densità discrete marginali** del vettore aleatorio $X$
>> ==La densità discreta congiunta determina le densità discrete marginali:==
>> Sia infatti $X = (X_{1} \dots X_{n})$ discreto, allora
>>$$
> > p_{X_{i}}(x) = \sum_{z\in \mathbb{R}^d:z_{i} = x}p_{X}(z) = \sum_{z\in \mathrm{Im}(X):z_{i} = x}p_{X}(z) \quad x \in \mathbb{R}
> > $$

> Nota:
> $$\mathrm{Im}(X) \subseteq \mathrm{Im}(X_{1}) \times \dots \times \mathrm{Im}(X_{d})$$

Quindi abbiamo per la densità marginale della prima componente:
$$
p_{X_{1}}(x) = \sum_{z_{2} \in \mathrm{Im}(X_{2})} \dots \sum_{z_{d} \in \mathrm{Im}(X_{d})} p_{X}((x,z_{2}, \dots z_{d})) \quad x \in \mathrm{Im}(X_{1})
$$

>[!example] Esempio
>Sia $X = (X_{1}, X_{2})$ un vettore bi-dimensionale a valori in $\{ 1 \dots 6 \} \times \{ 1 \dots 8 \}$ --> $|\mathrm{Im}(X)| = 48$
>Supponiamo che X abbia densità discreta data da
>$$
>p_{X}(z) = \begin{cases}
> \frac{1}{48} & \text{se } z \in \{ 1 \dots 6 \} \times \{ 1 \dots 8 \} \\
> 0 & \text{altrimenti}
>\end{cases} \quad z \in \mathbb{R}^2
>$$
>--> $p_{X}$ è la densità discreta congiunta di $X_{1}, X_{2}$
>--> Densità marginali
>$$
>p_{X_{1}}(x) = \sum_{z \in \mathbb{R}^2 : z_{1} = x} p_{X}(z) = \sum_{z_{2} \in \{ 1 \dots 8 \}} p_{X}((x, z_{2})) = 8 \cdot \frac{1}{48} = \frac{1}{6} \quad \text{se } x \in \{ 1 \dots 6 \}
>$$
>$p_{X_{1}}(x) = 0$ se $x \in \mathbb{R} \setminus \{ 1 \dots 6\}$ -> $p_{X_{1}}$ è la densità uniforme discreta su $\{ 1 \dots 6 \}$ 
>Analogamente si ottiene 
>$$
>p_{X_{2}}(x) = \begin{cases}
> 6 \cdot \frac{1}{48} = \frac{1}{8} & \text{se } x \in \{ 1 \dots 8 \} \\
> 0 & \text{altrimenti}
\end{cases}
>$$
>-> $p_{X_{2}}$ è la densità uniforme discreta su $\{ 1 \dots 8 \}$

>[!warning] Attenzione
>In generale, le densità discrete marginali **non** determinano la densità discreta congiunta

>[!example] Contro-Esempio
>Sia $X = (X_{1}, X_{2})$ un vettore aleatorio discreto bi-dimensionale con densità discreta data da
>$$
>p_{X}(z) = \begin{cases}
> \frac{1}{36} & \text{se } z \in \{ 1 \dots 6 \}^2 \\
> 0 & \text{altrimenti}
\end{cases}
>$$
>-> La densità discreta congiunta di $X_{1}, X_{2}$ è la densità uniforme discreta su $\{ 1 \dots 6 \}^2$
>-> Densità marginali di $X_{1}, X_{2}$: $X_{1} \sim \text{Unif}(\{ 1 \dots 6 \}), X_{2} \sim \{ 1 \dots 6 \}$
>Quindi:
>- Densità congiunta di $X_{1}, X_{2}$ è quella uniforme discreta su $\{ 1\dots 6 \}^2$ 
>- Densità marginali di $X_{1}, X_{2}$ sono uguali alla uniforme discreta su $\{ 1\dots 6 \}$ 
>
>Ora poniamo $\hat{X} := (X_{1}, X_{1})$
> - $\hat{X}_{1} = X_{1} \sim \text{Unif}(\{ 1\dots 6 \})$ 
> - $\hat{X}_{2} = X_{1} \sim \text{Unif}(\{ 1\dots 6 \})$ 
> 
> Le densità marginali di $\hat{X}_{1}, \hat{X}_{2}$ sono sempre uguali alla densità uniforme discreta $\{ 1 \dots 6 \}$
> Però la densità congiunta di $\hat{X}_{1}, \hat{X}_{2}$, **quindi la densità discreta** di $\hat{X}$, **non** è uguale a quella di $X = (X_{1}, X_{2})$
> $$
> \begin{align}
> p_{\hat{X}}(z) &= P(\hat{X}_{1} = z_{1}, \hat{X}_{2} = z_{2}) \\
> &= P(X_{1} = z_{1}, X_{1} = z_{2}) \\
> &= \begin{cases}
> 0 & \text{se } z_{1} \neq z_{2} \\
> P(X_{1} = z_{1}) & \text{se } z_{1} = z_{2}
\end{cases}
\end{align}
> $$
> ->
> $$
> p_{\hat{X}}(z) = \begin{cases}
> \frac{1}{6} & \text{se } z_{1} = z_{2} \in \{ 1 \dots 6 \} \\
> 0 & \text{altrimenti}
\end{cases}
> $$
> --> $\hat{X} \sim \text{Unif}(\{ z \in \{ 1 \dots 6 \}^2 : z_{1} = z_{2} \})$ invece $X \sim \text{Unif}(\{ 1 \dots 6 \}^2)$
> --> ==$X, \hat{X}$ **non** hanno la stessa densità congiunta anche se hanno le stesse densità marginali==.

Quindi abbiamo detto che, in generale, le densità marginali **non** determinano la densità congiunta, vedi contro esempio.
==Se invece le v.a. sono **indipendenti**, allora sì che la densità discreta congiunta è determinata dalle densità discrete marginali== infatti:
>[!info] Caratterizzazione dell'indipendenza delle v.a. discrete
>Siano $X_{1} \dots X_{n}$ v.a. discrete su $(\Omega, F, P)$
>Alloro sono equivalenti:
> 1. $X_{1} \dots X_{n}$ sono **indipendenti** come famiglia
> 2. $$
>    p_{(X_{1} \dots X_{n})}(z) = \prod_{i = 1}^n p_{X_{i}}(z_{i}) \quad z \in \mathbb{R}^n
>    $$
>    Dove:
>    - $p_{(X_{1} \dots X_{n})}$ -> Densità discreta congiunta (densità discreta del vettore aleatorio)
>    - $p_{X_{i}}(z_{i})$ -> Densità marginali (densità discrete dei componenti)
>3. $\forall$ scelta di $B_{1} \dots B_{n} \in \mathcal B(\mathbb{R})$ si ha
>   $$
>   P(X_{1} \in B_{1}, \dots, X_{n} \in B_{n}) = \prod_{i = 1}^n P(X_{i} \in B_{i})
>   $$
>
>>Nota:
>>1. --> 3. : $X_{1} \dots X_{n}$ indipendenti --> $p_{(X_{1}\dots X_{n})}(z)=P(X_{1} = z_{1} \dots X_{n} = z_{n}) = \prod_{i = 1}^nP(X_{i} = z_{i}) = \prod_{i = 1}^n p_{X_{i}}(z_{i})$

### Covarianza tra v.a.
>[!important] Obiettivo
>Misura importante per quantificare la "dipendenza" tra due v.a.


>[!def] Covarianza
>Siano $X,Y$ v.a. su $(\Omega, F, P)$ con momento secondo finito (cioè $E[X^2], E[Y^2] < \infty$)
>La **covarianza** di $X,Y$ è la quantità data da
>$$
>cov(X,Y) := E[(X - E[X]) \cdot (Y - E[Y])]
>$$
>>[!note] Osservazioni
>>1. Definizione analoga a quella della covarianza campionaria
>>   $$
>>   \frac{1}{n-1} \cdot \sum_{i = 1}^n (x_{i} - \overline{x}) \cdot (y_{i}-\overline{y})
>>   $$
>>   dove $\overline{x}, \overline{y}$ indicano le medie campionarie
>>
>>2. ==La covarianza prende valori in $\mathbb{R}$. Covarianza strettamente positiva indica che tendenzialmente due v.a. stanno insieme sopra o insieme sotto le rispettive medie; "movimenti opposti", invece, in caso di covarianza negativa.==
>>3. $var(X)=cov(X,X) \geq 0$
>>4. La covarianza dipende dalla distribuzione **congiunta**

#### Calcolo della covarianza per v.a. **discrete**
Siano $X,Y$ v.a. discrete con densità congiunta $p_{(X,Y)}$
Allora
$$
cov(X,Y) = \sum_{(x,y) \in \mathbb{R}^2}(x - E[X]) \cdot(y - E[Y]) \cdot p_{(X,Y)}(x,y)
$$
dove
$$
E[X] = \sum_{z \in \mathbb{R}} z \cdot p_{X}(z) \quad E[Y] = \sum_{z \in \mathbb{R}} z \cdot p_{Y}(z)
$$

>Ricorda:
>Densità marginali determinate dalla densità congiunta
>$$
>p_{X}(z) = \sum_{y \in \mathbb{R}}p_{(X,Y)}(z,y) \quad p_{Y}(z) = \sum_{x \in \mathbb{R}}p_{(X,Y)}(x, z)
>$$

>[!example] Lancio di due dadi
>Siano $Z_{1}, Z_{2}$ v.a. **indipendenti** e identicamente distribuite con comune distribuzione $\text{Unif}(\{ 1 \dots 6 \})$.
>Poniamo $X = Z_{1}, Y = Z_{1} + Z_{2}$, $cov(X,Y) = ?$. Ci aspettiamo $cov(X,Y) > 0$
>Densità discreta congiunta di $X, Y$:
>  
> | x \ y | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |  
> |:-------:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:----:|:----:|:----:|  
> | 1     | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | 0 | 0 | 0  | 0  | 0  |  
> | 2     | 0 | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | 0  | 0  | 0  |  
> | 3     | 0 | 0 | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | 0  | 0  | 0  |  
> | 4     | 0 | 0 | 0 | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | 0  | 0  |  
> | 5     | 0 | 0 | 0 | 0 | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | 0  |  
> | 6     | 0 | 0 | 0 | 0 | 0 | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ | $\frac{1}{36}$ |
>
>> Nota:
>> $P(Z_{1} = z_{1}, Z_{2} = z_{2}) = P(Z_{1} = z_{2}) \cdot P(Z_{2} = z_{2})$ pk $Z_{1}, Z_{2}$ sono indipendenti --> $P(Z_{2} = z_{2}) = P(Z_{2} = z_{2} | Z_{1} = z_{1})$ in quanto l'esito di $Z_{1}$ non influisce in alcun modo sull'esito di $Z_{2}$
>
>Inoltre:
>$$
>\begin{align}
> p_{(X,Y)}(x,y) &= P(X = x, Y = y) \\
> &= P(Y = y | X = x) \cdot P(X = x) \\
> &= P(Z_{1} + Z_{2} = y | Z_{1} = x) \\
> &= P(Z_{2} = y - x | Z_{1} = x) \\
> &= P(Z_{2} = y - x)
\end{align}
>$$
>--> $p_{(X,Y)}(x,y) = P(Z_{2} = y - x) \cdot P(Z_{1} = x)$
>Ora
>$$
>cov(X,Y) = \sum_{(x,y) \in \{ 1 \dots 6 \} \times \{ 2 \dots 12 \}} (x - E[X]) \cdot (y - E[Y]) \cdot p_{(X,Y)}(x,y)
>$$
>$E[X] = E[Z_{1}] = \frac{7}{2}$
>$E[Y] = E[Z_{1} + Z_{2}] = E[Z_{1}] + E[Z_{2}] = \frac{7}{2} + \frac{7}{2} = 7$
>--> 
>$$
>cov(X,Y) = \frac{1}{36} \cdot \left( \left( -\frac{5}{2} \right) \cdot (-5 + -4 + -3 + -2 + -1 + 0) \dots + \left( \frac{5}{2} \right) \cdot (0 + 1 + 2 + 3 + 4 + 5 + 6) \right) = \frac{35}{12}
>$$

### Proprietà Della Covarianza
Siano $X,Y,Z$ v.a. su $(\Omega, F, P)$ con momento secondo finito.
1. **Simmetria**: $cov(X,Y) = cov(Y,X)$
2. **Bi-linearità**: $\forall \alpha,\beta \in \mathbb{R}$
   $$
   \begin{align}
cov(\alpha X + \beta Z, Y) &= \alpha \cdot cov(X,Y) + \beta \cdot cov(Z,Y) \\
cov(X, \alpha Y + \beta Z) &= \alpha \cdot cov(X,Y) + \beta cov(X,Z)
\end{align}
   $$

3. $cov(X,Y) = E[X \cdot Y] - E[X] \cdot E[Y]$
4. Se $X,Y$ sono **indipendenti**, allora $cov(X,Y) = 0$
   Due v.a. si dicono **incorrelate** se $cov(X,Y) = 0$
   Dunque
   $$
   X, Y \text{ indipendenti } \to X,Y \text{ incorellate}
   $$
   >[!warning] Attenzione
   >L'implicazione inversa:
   >$$
   > X,Y \text{ incorrelate} \to X, Y \text{ indipendenti}
   >$$
   >**non** vale in generale
   >==Quindi v.a. incorrelate **non** necessariamente indipendenti==
   >>[!example] Esempio
   >>Sia $Z \sim \text{Unif}(\{ -1, 0, 1 \})$.
   >>Poniamo:
   >>$$
   >>X := \perp_{\{ 0 \}}(Z) = \begin{cases}
> > 1 & \text{se } Z = 0 \\
> > 0 & \text{altrimenti}
> >\end{cases}, \quad Y := Z
   >>$$
   >>Allora $E[X] = 1 \cdot \frac{1}{3} = \frac{1}{3}$, $E[Y]=\frac{1}{3} \cdot (-1 + 0+1) = 0$
   >>-->
   >>$$
   >>\begin{align}
> > cov(X,Y) &= E\left[ \left( X-\frac{1}{3} \right) \cdot Y \right] \\
> > \text{(per linearità) } &= E[X \cdot Y] - \frac{1}{3} \cdot E[Y] \\
> > &= E[X \cdot Y] \\
> > &= E[\perp_{\{ 0 \}}(z) \cdot Z] \\
> > &= \frac{1}{3} \cdot (\perp_{\{ 0 \}}(-1) \cdot (-1) + \perp_{\{ 0 \}}(0) \cdot (0)+ \perp_{\{ 0 \}}(1) \cdot (1)) = 0 \\
> > cov(X,Y) = 0
> >\end{align}
   >>$$
   >>Però: $X,Y$ **non** indipendenti; ad esempio: $P(X = 1, Y = 1) = 0$, mentre $P(X=1) = \frac{1}{3}, P(Y = 1) = \frac{1}{3}$ --> $P(X = 1, Y = 1) \neq P(X = 1) \cdot P(Y = 1)$

## Varianza Della Somma di V.A. ([[7.1.2 Varianza V.A.]])
Siano $X_{1}\dots X_{n}$ v.a. su $(\Omega,F ,P)$ con momento secondo finito
Allora
$$
\begin{align}
var\left( \sum_{i = 1}^n X_{i} \right) &= cov\left( \sum_{i = 1}^n X_{i}, \sum_{j = 1}^n X_{j} \right) \\
\text{(bilinearità)} &= \sum_{i = 1}^n cov\left(X_{i}, \sum_{j = 1}^n X_{j} \right) \\
\text{(bilinearità)} &= \sum_{i,j = 1}^n cov(X_{i}, X_{j}) \\
&= \sum_{i = 1}^n var(X_{i}) + \sum_{i,j = 1:i\neq j}^n cov(X_{i}, X_{j}) \\
\text{(simmetria}:cov(X_{i},X_{j}) = cov(X_{j},X_{i}) \text{)} &= \sum_{i = 1}^n var(X_{i}) + 2\cdot \sum_{1 \leq i < j \leq n} cov(X_{i}, X_{j})
\end{align}
$$
Quindi:
$$
var\left( \sum_{i = 1}^n X_{i}\right) = \sum_{i = 1}^n var(X_{i}) + 2 \cdot \sum_{1 \leq i < j \leq n}^n cov(X_{i}, X_{j})
$$
==Se però $X_{1} \dots X_{n}$ sono **incorrelate a due a due oppure sono indipendenti** allora==
$$
var\left( \sum_{i = 1}^n X_{i} \right) = \sum_{i = 1}^n var(X_{i})
$$

### Applicazione: Passeggiata aleatoria semplice e simmetrica
Siano $\xi_{1}, \xi_{2}\dots$ v.a. su $(\Omega, F, P)$ indipendenti e identicamente distribuite (**i.i.d**) con comune distribuzione di Rademacher di parametro $\frac{1}{2}$, quindi $\xi_{1}, \xi_{2}\dots$ indipendenti e $P(\xi_{i} = 1) = \frac{1}{2} = P(\xi_{i} = -1) \quad \forall i \in \mathbb{N}$
Poniamo
$$
\begin{align}
S_{0} &:= 0 \\
S_{n} &:= \sum_{i = 1}^n \xi_{i} \quad n \in \mathbb{N}
\end{align}
$$
Interpretazioni:
1. $S_n$ da la posizione dopo $n$ passi di una particella che parte da zero e a ogni passo si sposta di un'unità a destra o sinistra in maniera casuale con uguale probabilità ("passeggiata simmetrica")
2. $S_{n}$ dà il guadagno dopo $n$ giocate / scommesse di un gioco "leale" in cui la posta in ogni giocata è di un'unità (1€).

> Nota: 
> -> Posizione media dopo $n$ passi
> $$
> E[S_{n}] = \sum_{i = 1}^nE[\xi_{i}] = 0
> $$
> -> Varianza
> $$
> var(S_{n})=var\left( \sum_{i = 1}^n \xi_{i} \right) =^{\text{indipendenza dei }\xi_{i}} \sum_{i = 1}^nvar(\xi_{i}) = n
> $$
> Dalla disuguaglianza di Markov-Chebyshev: $\forall \epsilon > 0$
> $$
> P(|S_{n}| \geq \epsilon) \leq \frac{var(S_{n})}{\epsilon^2} = \frac{n}{\epsilon^2}
> $$
> Questa stima diventa significativa per $\epsilon > \sqrt{ n }$ dove $\sqrt{ n } = \sqrt{ var(S_{n}) }$ deviazione standard di $S_{n}$
> --> valori tipici di $S_{n}$ in $[-c \cdot \sqrt{ n }, c \cdot \sqrt{ n }]$ per $c \geq 1$; ad esempio, con $c = 5$
> $$
> P(S_{n} \in [-5 \sqrt{ n }, 5 \sqrt{ n }]) \geq 96\%
> $$

---
Come per i campioni bivariati, possiamo definire il coefficiente di correlazione tra v.a.
>[!def] Coefficiente di Correlazione tra $X,Y$
>Siano $X,Y$ v.a. su $(\Omega, F,P)$ con $var(X),var(Y) \in (0, \infty)$
>Il **coefficiente di correlazione** tra $X$ e $Y$ è dato da
>$$
>\rho(X,Y) := \frac{cov(X,Y)}{\sqrt{ var(X) }\cdot \sqrt{ var(Y) }}
>$$
>>[!info] Osservazioni
>>1. $\rho(X,Y)$ non dipende dall'unità di misura:
>>   $$\rho(a \cdot X + b, Y) = \rho(X,Y) \quad \forall a>0, b \in \mathbb{R}$$
>>   Inoltre $\rho(X,Y)=\rho(Y,X)$
>>2. $\rho(X,Y) \in [-1,1]$
>>   - $\rho(X,Y) = 1$ **se e solo se** esistono $a > 0, b \in \mathbb{R}$ tali che $Y = a \cdot X + b$
>>   - $\rho(X,Y) = -1$ **se e solo se** esistono $a < 0, b \in \mathbb{R}$ tali che $Y = a \cdot X + b$
>>   - $\rho(X,Y) = 0$ **se e solo se** $X,Y$ sono incorrelate

>[!example] Due dadi
>$Z_{1}, Z_{2}$ **indipendenti** con $Z_{i} \sim \text{Unif}(\{ 1 \dots 6 \})$
>Poniamo:
>- $X := Z$
>- $Y := Z_{1} + Z_{2}$
>
>Allora $cov(X,Y) = cov(Z_{1}, Z_{1} + Z_{2}) =^{\text{bilinearità}}cov(Z_{1},Z_{2}) + cov(Z_{1},Z_{2})$ dove $cov(Z_{1},Z_{2}) = 0$ pk indipendenti
>--> $cov(X,Y) = var(Z_{1}) = var(X)$
>$$
>\begin{align}
>var(Z_{1}) &= E[Z_{1}^2] - E[Z_{1}]^2 \\
> E[Z_{1}] &= \frac{1}{6} \cdot \left( \sum_{i = 1}^6 i \right) = \frac{7}{2}
> E[Z_{1}^2] &= \frac{1}{6} \cdot \left( \sum_{i = 1}^6 i^2 \right) = \frac{91}{6} \\
>var(Z_{1}) &= \frac{91}{6} - \frac{49}{4} = \frac{35}{12}
\end{align}
>$$
>--> $cov(X,Y) = var(Z_{1}) = var(X)=\frac{35}{12}$
>Ora voglio sapere $var(Y) = var(Z_{1}+Z_{2}) =^{Z_{1},Z_{2} \text{ indipendenti}} var(Z_{1}) + var(Z_{2})$
>--> $var(Y) =^{Z_{1}, Z_{2}\text{ hanno la stessa distribuzione}} 2var(Z_{1}) = \frac{35}{6}$
>Dunque il coefficiente di correlazione è:
>$$
>\rho(X,Y) = \frac{\frac{35}{12}}{\sqrt{ \frac{35}{12} } \cdot \sqrt{ \frac{36}{6} }} = \frac{1}{\sqrt{ 2 }}
>$$